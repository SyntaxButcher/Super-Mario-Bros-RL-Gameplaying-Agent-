{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "#torch imports\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch import nn\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "# Gym is an OpenAI toolkit for RL\n",
    "import gym\n",
    "from gym.spaces import Box\n",
    "from gym.wrappers import FrameStack\n",
    "\n",
    "# NES Emulator for OpenAI Gym\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "\n",
    "# Super Mario environment for OpenAI Gym\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import RIGHT_ONLY\n",
    "\n",
    "# For Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For storing lists and reusing\n",
    "import pickle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        done = False\n",
    "        for i in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return obs, total_reward, done, info\n",
    "\n",
    "\n",
    "class GrayScaleObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.observation_space = Box(low=0, high=255, shape=self.observation_space.shape[:2], dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        transform = transforms.Grayscale()\n",
    "        return transform(torch.tensor(np.transpose(observation, (2, 0, 1)).copy(), dtype=torch.float))\n",
    "\n",
    "\n",
    "class ResizeObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env, shape):\n",
    "        super().__init__(env)\n",
    "        self.shape = (shape, shape)\n",
    "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        transformations = transforms.Compose([transforms.Resize(self.shape), transforms.Normalize(0, 255)])\n",
    "        return transformations(observation).squeeze(0)\n",
    "\n",
    "env = gym_super_mario_bros.make('SuperMarioBros-1-1-v3')\n",
    "env = SkipFrame(env, skip=4)\n",
    "env = GrayScaleObservation(env)\n",
    "env = ResizeObservation(env, shape=84)\n",
    "env = FrameStack(env, num_stack=4)\n",
    "env = JoypadSpace(env, RIGHT_ONLY)\n",
    "\n",
    "env2 = env\n",
    "# env.seed(42)\n",
    "# env.action_space.seed(42)\n",
    "# torch.manual_seed(42)\n",
    "# torch.random.manual_seed(42)\n",
    "# np.random.seed(42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a Convolutional Network for Actor and Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=4, out_channels=32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3136, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, env.action_space.n)\n",
    "        )\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=4, out_channels=32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3136, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, obs):\n",
    "        return Categorical(logits=self.actor(obs)), self.critic(obs).reshape(-1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOAgent:\n",
    "    def __init__(self,params):\n",
    "        #device, we use gpu\n",
    "        self.device = torch.device(\"cuda\")\n",
    "        #params\n",
    "        self.params = params\n",
    "        self.rewards = []\n",
    "        self.gamma = params['gamma']\n",
    "        self.lamda = params['lamda']\n",
    "        self.n_mini_batch = params['n_mini_batch']\n",
    "        self.epochs = params['epochs']\n",
    "        self.save_directory = params['save_directory']\n",
    "        self.batch_size = params['batch_size']\n",
    "        self.mini_batch_size = self.batch_size // self.n_mini_batch\n",
    "        self.state = env.reset().__array__()\n",
    "        self.policy = CNNet().to(self.device)\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.optimizer = torch.optim.Adam([\n",
    "            {'params': self.policy.actor.parameters(), 'lr': 0.00025},\n",
    "            {'params': self.policy.critic.parameters(), 'lr': 0.001}\n",
    "        ], eps=1e-4)\n",
    "        self.policy_old = CNNet().to(self.device)\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "        self.undiscounted_rewards = []\n",
    "        self.all_mean_rewards = []\n",
    "        self.episode = 0\n",
    "        self.visual = params['visual']\n",
    "        self.max_episodes = params['episodes']\n",
    "    \n",
    "    def sample(self):\n",
    "\n",
    "        # Initiating environment variables\n",
    "        rewards = np.zeros(self.batch_size, dtype=np.float32)\n",
    "        actions = np.zeros(self.batch_size, dtype=np.int32)\n",
    "        done = np.zeros(self.batch_size, dtype=bool)\n",
    "        state = np.zeros((self.batch_size, 4, 84, 84), dtype=np.float32)\n",
    "        log_pis = np.zeros(self.batch_size, dtype=np.float32)\n",
    "        values = np.zeros(self.batch_size, dtype=np.float32)\n",
    "\n",
    "        for t in range(self.batch_size):\n",
    "            # temporarily setting all requires_grad flags to false\n",
    "            with torch.no_grad():\n",
    "                state[t] = self.state\n",
    "                actionProb, v = self.policy_old(torch.tensor(self.state, dtype=torch.float32, device=self.device).unsqueeze(0))\n",
    "                values[t] = v.cpu().numpy()\n",
    "                #taking action from distribution\n",
    "                action = actionProb.sample()\n",
    "                actions[t] = action.cpu().numpy()\n",
    "                log_pis[t] = actionProb.log_prob(action).cpu().numpy()\n",
    "            self.state, rewards[t], done[t], _ = env.step(actions[t])\n",
    "            self.state = self.state.__array__()\n",
    "            if self.visual == True:\n",
    "                env.render()\n",
    "            self.rewards.append(rewards[t])\n",
    "            if done[t]:\n",
    "                self.episode += 1\n",
    "                self.undiscounted_rewards.append(np.sum(self.rewards))\n",
    "                self.rewards = []\n",
    "                env.reset()\n",
    "                if self.episode % 10 == 0:\n",
    "                    print('Episode: {}, average reward: {}'.format(self.episode, np.mean(self.undiscounted_rewards[-10:])))\n",
    "                    self.all_mean_rewards.append(np.mean(self.undiscounted_rewards[-10:]))\n",
    "                    plt.plot(self.all_mean_rewards)\n",
    "                    plt.savefig(\"{}/Unidiscounted_Reward_ep_{}.png\".format(self.save_directory, self.episode))\n",
    "                    plt.clf()\n",
    "                    self.save_model()\n",
    "\n",
    "            # episode limiter\n",
    "            if self.episode >= self.max_episodes:\n",
    "                break\n",
    "            \n",
    "        returns, advantages = self.calculate_advantages(done, rewards, values)\n",
    "        \n",
    "        # with open('returns_ppo.pkl', 'wb') as file:\n",
    "        #     pickle.dump(returns, file)\n",
    "        \n",
    "        # with open('rewards_ppo.pkl', 'wb') as file:\n",
    "        #     pickle.dump(self.all_rewards, file)\n",
    "\n",
    "        return {\n",
    "            'states': torch.tensor(state.reshape(state.shape[0], *state.shape[1:]), dtype=torch.float32, device=self.device),\n",
    "            'actions': torch.tensor(actions, device=self.device),\n",
    "            'values': torch.tensor(values, device=self.device),\n",
    "            'log_pis': torch.tensor(log_pis, device=self.device),\n",
    "            'advantages': torch.tensor(advantages, device=self.device, dtype=torch.float32),\n",
    "            'returns': torch.tensor(returns, device=self.device, dtype=torch.float32)\n",
    "        }\n",
    "\n",
    "    def calculate_advantages(self, done, rewards, values):\n",
    "        _, last_value = self.policy_old(torch.tensor(self.state, dtype=torch.float32, device=self.device).unsqueeze(0))\n",
    "        last_value = last_value.cpu().data.numpy()\n",
    "        values = np.append(values, last_value)\n",
    "        returns = []\n",
    "        gae = 0\n",
    "        for i in reversed(range(len(rewards))):\n",
    "            mask = 1.0 - done[i]\n",
    "            delta = rewards[i] + self.gamma * values[i + 1] * mask - values[i]\n",
    "            gae = delta + self.gamma * self.lamda * mask * gae\n",
    "            returns.insert(0, gae + values[i])\n",
    "        adv = np.array(returns) - values[:-1]\n",
    "        return returns, (adv - np.mean(adv)) / (np.std(adv) + 1e-8)\n",
    "\n",
    "    def calculate_loss(self, samples, clip_range):\n",
    "        sampled_returns = samples['returns']\n",
    "        sampled_advantages = samples['advantages']\n",
    "        actionProb, value = self.policy(samples['states'])\n",
    "        ratio = torch.exp(actionProb.log_prob(samples['actions']) - samples['log_pis'])\n",
    "        clipped_ratio = ratio.clamp(min=1.0 - clip_range, max=1.0 + clip_range)\n",
    "        policy_reward = torch.min(ratio * sampled_advantages, clipped_ratio * sampled_advantages)\n",
    "        entropy_bonus = actionProb.entropy()\n",
    "        vf_loss = self.mse_loss(value, sampled_returns)\n",
    "        loss = -policy_reward + 0.5 * vf_loss - 0.01 * entropy_bonus\n",
    "        return loss.mean()\n",
    "\n",
    "    def train(self, samples, clip_range):\n",
    "        indexes = torch.randperm(self.batch_size)\n",
    "        for start in range(0, self.batch_size, self.mini_batch_size):\n",
    "            end = start + self.mini_batch_size\n",
    "            mini_batch_indexes = indexes[start: end]\n",
    "            mini_batch = {}\n",
    "            for k, v in samples.items():\n",
    "                mini_batch[k] = v[mini_batch_indexes]\n",
    "            for _ in range(self.epochs):\n",
    "                loss = self.calculate_loss(clip_range=clip_range, samples=mini_batch)\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "            self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "\n",
    "    def save_model(self):\n",
    "        filename = os.path.join(self.save_directory, 'PPOmodel_ep_{}.pth'.format(self.episode))\n",
    "        torch.save(self.policy_old.state_dict(), f=filename)\n",
    "        #print('Model saved to \\'{}\\''.format(filename))\n",
    "\n",
    "    def load_model(self, filename):\n",
    "        self.policy.load_state_dict(torch.load(os.path.join(self.save_directory, filename)))\n",
    "        self.policy_old.load_state_dict(torch.load(os.path.join(self.save_directory, filename)))\n",
    "        #print('Resuming training from checkpoint \\'{}\\'.'.format(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shaks\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pyglet\\image\\codecs\\wic.py:289: UserWarning: [WinError -2147417850] Cannot change thread mode after it is set\n",
      "  warnings.warn(str(err))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 10, average reward: 696.9000244140625\n"
     ]
    }
   ],
   "source": [
    "train_parameters = {\n",
    "    'gamma' : 0.99,\n",
    "    'lamda' : 0.95,\n",
    "    'n_mini_batch' : 4,\n",
    "    'epochs' : 30,\n",
    "    'save_directory' : \"./Models\",\n",
    "    'batch_size' : 4096,\n",
    "    'visual' : True,\n",
    "    'episodes' : 10,\n",
    "}\n",
    "\n",
    "# The eps value for 1-eps : 1+eps clipping\n",
    "PPO_clip = 0.2\n",
    "\n",
    "solver = PPOAgent(train_parameters)\n",
    "while True:\n",
    "    solver.train(solver.sample(), PPO_clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # set the random seed\n",
    "    # np.random.seed(1234)\n",
    "    # random.seed(1234)\n",
    "    # torch.manual_seed(1234)\n",
    "\n",
    "\n",
    "    # create training parameters\n",
    "    train_parameters = {\n",
    "    'gamma' : 0.99,\n",
    "    'lamda' : 0.95,\n",
    "    'n_mini_batch' : 4,\n",
    "    'epochs' : 30,\n",
    "    'save_directory' : \"./mario_ppo\",\n",
    "    'batch_size' : 4096,\n",
    "    'visual' : False,\n",
    "    'episodes' : 10,\n",
    "}\n",
    "\n",
    "    agent = PPOAgent(train_parameters)\n",
    "    agent.load_model(\"marioppo.pth\")\n",
    "\n",
    "    gamma = 0.99\n",
    "    step_list = []\n",
    "    undisc_returns = []\n",
    "    returns = []\n",
    "    for i in range(100):\n",
    "        state = env2.reset().__array__()\n",
    "        step_count = 0\n",
    "        done = False\n",
    "        ep_reward = []\n",
    "        R = 0\n",
    "        G = 0\n",
    "        while not done:\n",
    "            actionProb,_ = agent.policy_old(torch.tensor(state, dtype=torch.float32, device=torch.device(\"cuda\")).unsqueeze(0))\n",
    "            action = actionProb.sample()\n",
    "            actiond = np.zeros(1, dtype = np.int32)\n",
    "            actiond[0] = action.cpu().numpy()\n",
    "            next_obs, reward, done, info = env2.step(actiond[0])\n",
    "            flag = info['flag_get']\n",
    "            ep_reward.append(reward)\n",
    "            step_count+= 1\n",
    "            state = next_obs.__array__()\n",
    "        if flag:\n",
    "            for r in reversed(ep_reward):\n",
    "                R += r\n",
    "                G  = r + gamma*G\n",
    "            undisc_returns.append(R)\n",
    "            returns.append(G)\n",
    "            step_list.append(step_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('trained_returns_ppo.pk1', 'wb') as file:\n",
    "    pickle.dump(returns, file)\n",
    "\n",
    "with open('undisc_trained_returns_ppo.pk1', 'wb') as file:\n",
    "    pickle.dump(undisc_returns, file)\n",
    "\n",
    "with open('step_count_ppo.pk1', 'wb') as file:\n",
    "    pickle.dump(step_list, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env2.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2a44a310d340e19f6e12308d6e1160a19de9f91ffa5c15d609e6c86191fac432"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
